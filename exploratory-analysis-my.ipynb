{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "Group project for the 2019 Data Science Workshop at the University of California, Berkeley.\n",
    "\n",
    "The project is the Google Analytics Customer Revenue Prediction competition on Kaggle: https://www.kaggle.com/c/ga-customer-revenue-prediction\n",
    "\n",
    "Group members:\n",
    "\n",
    "* Andy Vargas (mentor)\n",
    "* Yuem Park\n",
    "* Marvin Pohl\n",
    "* Michael Yeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import pandas_profiling\n",
    "from pandas.io.json import json_normalize\n",
    "import time\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:\n",
    "\n",
    "Note that the data files are too large to upload to GitHub - instead, the directory `./data/` has been added to the .gitignore, which should contain the following files on your local machine, all downloaded from the Kaggle competition website:\n",
    "\n",
    "* sample_submission_v2.csv\n",
    "* test_v2.csv\n",
    "* train_v2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def hits_converter(data):\n",
    "#    return json.loads(json.dumps(ast.literal_eval(data)))\n",
    "\n",
    "#def customDimensions_converter(data):\n",
    "#    if data == '[]':\n",
    "#        return {}\n",
    "#    else:\n",
    "#        return hits_converter(data)[0]\n",
    "\n",
    "#too slow. Faster to load data, then convert columns to appropriate format.\n",
    "#def load_df1(csv_path='data/train_v2.csv', nrows=None, skiprows=None):\n",
    "#    conv_dict = {'device': ujson.loads,\n",
    "#                'geoNetwork': ujson.loads,\n",
    "#                'totals': ujson.loads,\n",
    "#                'trafficSource': ujson.loads,\n",
    "#                'hits': hits_converter,\n",
    "#                'customDimensions': customDimensions_converter}\n",
    "#    df = pd.read_csv(csv_path, \n",
    "#                     converters=conv_dict, \n",
    "#                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "#                     nrows=nrows)\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path, nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_converter(yyyymmdd):\n",
    "    #convert date from integer to datetime object\n",
    "    return pd.to_datetime(yyyymmdd, format='%Y%m%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicer(start_date, num_days, csv_path = 'data/train_v2.csv'):\n",
    "    #get rows whose dates lie in the num_days-long period beginning on start_date\n",
    "    #input start_date as an integer YYYYMMDD\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d').date()\n",
    "    num_days = datetime.timedelta(num_days)\n",
    "    reader = pd.read_csv(csv_path, chunksize=100000,\n",
    "                         converters = {'date': date_converter},\n",
    "                         dtype={'fullVisitorId': 'str'})\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    for chunk in reader:\n",
    "        chunk = chunk[(chunk['date'] >= start_date) & (chunk['date'] < start_date + num_days)]\n",
    "        chunks.append(chunk)\n",
    "        i+=1\n",
    "        print(f\"Processed {i} chunks.\")\n",
    "    df = pd.concat(chunks)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample = slicer(20170128, 168)\n",
    "sample = load_df('data/train_1-28-17_raw.csv')\n",
    "#Loaded train_1-28-17_raw.csv. Shape: (369028, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = slicer(20170830, 62)\n",
    "test.shape\n",
    "#(191863, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train:\n",
    "    if train[column].nunique() == 1:\n",
    "        train = train.drop(column, axis=1)\n",
    "\n",
    "train=train.drop(['totals.totalTransactionRevenue', 'totals.transactionRevenue'], axis=1)\n",
    "train=train.drop(['Unnamed: 0', 'hits', 'customDimensions'], axis=1)\n",
    "\n",
    "target_copy = target[['fullVisitorId', 'totals.totalTransactionRevenue', 'totals.transactionRevenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_pickle('data/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369028, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=pd.read_pickle('data/target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191863, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
