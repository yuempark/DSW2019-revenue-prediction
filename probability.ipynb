{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "Group project for the 2019 Data Science Workshop at the University of California, Berkeley.\n",
    "\n",
    "The project is the Google Analytics Customer Revenue Prediction competition on Kaggle: https://www.kaggle.com/c/ga-customer-revenue-prediction\n",
    "\n",
    "Group members:\n",
    "\n",
    "* Andy Vargas (mentor)\n",
    "* Yuem Park\n",
    "* Marvin Pohl\n",
    "* Michael Yeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import time\n",
    "import os\n",
    "import datetime as dt\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:\n",
    "\n",
    "Note that the data files are too large to upload to GitHub - instead, the directory `./data/` has been added to the .gitignore, which should contain the following files on your local machine, all downloaded from the Kaggle competition website:\n",
    "\n",
    "* sample_submission_v2.csv\n",
    "* test_v2.csv\n",
    "* train_v2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path, nrows=None):\n",
    "    #from someone's Kaggle kernel. Loads data and flattens JSON columns.\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_converter(yyyymmdd):\n",
    "    #convert date from integer to datetime object\n",
    "    return pd.to_datetime(yyyymmdd, format='%Y%m%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicer(start_date, num_days, csv_path = 'data/train_v2.csv'):\n",
    "    #get rows whose dates lie in the num_days-long period beginning on start_date\n",
    "    #input start_date as an integer YYYYMMDD\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d').date()\n",
    "    num_days = dt.timedelta(num_days)\n",
    "    reader = pd.read_csv(csv_path, chunksize=100000,\n",
    "                         converters = {'date': date_converter},\n",
    "                         dtype={'fullVisitorId': 'str'})\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    for chunk in reader:\n",
    "        chunk = chunk[(chunk['date'] >= start_date) & (chunk['date'] < start_date + num_days)]\n",
    "        chunks.append(chunk)\n",
    "        i+=1\n",
    "        print(f\"Processed {i} chunks.\")\n",
    "    df = pd.concat(chunks)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df, var_name):\n",
    "    #for each fullVisitorId, count instances of each value of a categorical variable named var_name\n",
    "    #input is dataframe with only two columns (fullVisitorId and var_name)\n",
    "    #returns dataframe in which each column is a count of a single value, index = fullVisitorId\n",
    "    df = df.pivot_table(index='fullVisitorId', columns=var_name, aggfunc=len, fill_value=0)\n",
    "    df.columns = [f\"{var_name}.{col}\" for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_constant(df):\n",
    "    #drop constant columns\n",
    "    for column in df:\n",
    "        if df[column].nunique(dropna=False) == 1:\n",
    "            df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(series, n):\n",
    "    #the values of series which have the n largest value counts are kept,\n",
    "    #and all other values are changed to \"other\"\n",
    "    series=series.to_frame()\n",
    "    value_counts_rk = series.apply(lambda x: x.map(x.value_counts().rank(ascending=False, method='min')))\n",
    "    return series.where(value_counts_rk <= n, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data processing\n",
    "train=slicer(20170501, 168)\n",
    "train['date'].max()\n",
    "#train.to_csv('data/train_5-1-17.csv')\n",
    "train=load_df('data/train_5-1-17.csv')\n",
    "#train.to_pickle('data/train_5-1-17_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target dataset processing\n",
    "target=slicer(20171201, 62, csv_path = 'data/train_v2.csv')\n",
    "target['date'].max()\n",
    "#datetime.date(2018, 1, 31)\n",
    "\n",
    "#target.to_csv('data/target_12-1-17.csv')\n",
    "#target=load_df('data/target_12-1-17.csv')\n",
    "#Loaded target_12-1-17.csv. Shape: (180494, 60)\n",
    "\n",
    "#target.to_pickle('data/target_12-1-17_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute target for logistic regression\n",
    "positive_revenue_ids = target[target['totals.transactionRevenue'].fillna(value=0).astype('float') > 0]['fullVisitorId']\n",
    "train_ids = train['fullVisitorId'].drop_duplicates().to_frame()\n",
    "logistic_target = train_ids.assign(\n",
    "    **{'target': train_ids['fullVisitorId'].isin(set(positive_revenue_ids)).apply(int)})\n",
    "logistic_target = logistic_target.set_index('fullVisitorId')\n",
    "#logistic_target.to_pickle('data/logistic_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#format training data\n",
    "train=pd.read_pickle('data/train_5-1-17_raw.pkl')\n",
    "train=drop_constant(train)\n",
    "train = train.drop('Unnamed: 0', axis=1)\n",
    "train['trafficSource.isTrueDirect'] = train['trafficSource.isTrueDirect'].fillna(value=False)\n",
    "fill_in_cols = ['totals.hits', 'totals.pageviews', 'totals.bounces', 'totals.newVisits', 'totals.timeOnSite',\n",
    "                'totals.transactions', 'totals.transactionRevenue', 'totals.totalTransactionRevenue',\n",
    "                'totals.sessionQualityDim']\n",
    "train[fill_in_cols] = train[fill_in_cols].fillna(value = 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take log of transaction revenue\n",
    "log_rev = np.log(train[['totals.transactionRevenue', 'totals.totalTransactionRevenue']]+1)\n",
    "log_rev = log_rev.rename(\n",
    "    mapper = {'totals.transactionRevenue': 'log_tr', 'totals.totalTransactionRevenue': 'log_ttr'}, axis=1)\n",
    "train = train.join(log_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feats = ['visitNumber',#max\n",
    "                  'totals.hits',#sum\n",
    "                  'totals.pageviews',#sum\n",
    "                  'totals.bounces',#sum\n",
    "                  'totals.timeOnSite',#sum\n",
    "                  'totals.transactions',#sum\n",
    "                  'totals.transactionRevenue',#by month\n",
    "                  'totals.totalTransactionRevenue',#by month\n",
    "                  'totals.sessionQualityDim']#avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = ['channelGrouping',\n",
    "                     'visitStartTime', #morning, afternoon, evening, night\n",
    "                    'device.browser',\n",
    "                    'device.operatingSystem',\n",
    "                    'device.deviceCategory',\n",
    "                    'geoNetwork.continent',\n",
    "                    'geoNetwork.subContinent',\n",
    "                    'trafficSource.isTrueDirect',\n",
    "                    'trafficSource.referralPath',\n",
    "                    'trafficSource.adContent',\n",
    "                    'trafficSource.adwordsClickInfo.page',\n",
    "                    'trafficSource.adwordsClickInfo.slot',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cols = ['customDimensions', 'date', 'fullVisitorId', 'hits', 'visitId','device.isMobile',\n",
    "              'geoNetwork.country',\n",
    "              'geoNetwork.region',\n",
    "              'geoNetwork.metro',\n",
    "              'geoNetwork.city',\n",
    "              'geoNetwork.networkDomain',\n",
    "              'totals.newVisits',\n",
    "              'trafficSource.campaign',\n",
    "              'trafficSource.source',\n",
    "              'trafficSource.medium',\n",
    "              'trafficSource.adwordsClickInfo.gclId',\n",
    "              'trafficSource.adwordsClickInfo.adNetworkType',\n",
    "              'trafficSource.adwordsClickInfo.isVideoAd',\n",
    "              'trafficSource.keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numerical features\n",
    "df = train['fullVisitorId'].drop_duplicates().to_frame()\n",
    "\n",
    "df = df.join(train[['fullVisitorId', 'visitNumber']].groupby('fullVisitorId').max(), on='fullVisitorId')\n",
    "\n",
    "sum_feats = ['totals.hits', 'totals.pageviews', 'totals.bounces', 'totals.timeOnSite', 'totals.transactions']\n",
    "df = df.join(train[['fullVisitorId']+sum_feats].groupby('fullVisitorId').sum(), on='fullVisitorId')\n",
    "\n",
    "df = df.join(train[['fullVisitorId', 'totals.sessionQualityDim']].groupby('fullVisitorId').mean(), on='fullVisitorId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add transaction revenue by month as feature\n",
    "month_df = train[['fullVisitorId', 'date', 'totals.transactionRevenue',\n",
    "                  'totals.totalTransactionRevenue', 'log_tr', 'log_ttr']]\n",
    "month_df.loc[:,'date'] = month_df.loc[:,'date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d').month)\n",
    "month_df = month_df.rename(mapper={'date': 'month'}, axis=1)\n",
    "month_df = month_df.groupby(['fullVisitorId', 'month']).sum().unstack(fill_value=0)\n",
    "month_df.columns = month_df.columns.to_flat_index()\n",
    "df = df.join(month_df, on='fullVisitorId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(hour):\n",
    "    if hour >= 5 and hour < 12:\n",
    "        return 'morning'\n",
    "    if hour >= 12 and hour < 18:\n",
    "        return 'afternoon'\n",
    "    if hour >= 18 and hour < 23:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn visitStartTime into categorical feature 'visit time of day'\n",
    "visit_tod = pd.to_datetime(train['visitStartTime'], unit='s').dt.hour.apply(time_of_day)\n",
    "train = train.drop('visitStartTime', axis=1)\n",
    "train = train.join(visit_tod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add in categorical features\n",
    "for col in categorical_feats:\n",
    "    if col == 'geoNetwork.subContinent':\n",
    "        temp_df = train['fullVisitorId'].to_frame().join(train[col].to_frame())\n",
    "        temp_df = featurize(temp_df, col)\n",
    "        df = df.join(temp_df, on='fullVisitorId')\n",
    "        print(f\"Finished {col}. Size: {df.shape}.\")\n",
    "    else:\n",
    "        temp_df = train['fullVisitorId'].to_frame().join(truncate(train[col], n=10))\n",
    "        temp_df = featurize(temp_df, col)\n",
    "        df = df.join(temp_df, on='fullVisitorId')\n",
    "        print(f\"Finished {col}. Size: {df.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('fullVisitorId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OOPS! This feature does not appear in the test data. Should've processed training data differently to account for this.\n",
    "#As written, the features extracted from the training and test data are based on values of categorical variables that\n",
    "#actually appear. Should have extracted features from training data, then compute these features for the test data in \n",
    "#a \"hard-coded\" way\n",
    "X1 = df.iloc[:,0:97].drop('device.operatingSystem.BlackBerry', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.to_pickle('data/reduced_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic model with log-revenue, without trafficSource columns, trained on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/train.pkl')\n",
    "target = pd.read_pickle('data/logistic_target.pkl')['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regr = LogisticRegression(solver = 'lbfgs', max_iter = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = X1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scales, 'data/reduced_scales.joblib')\n",
    "#normalize features so that logistic regression converges more quickly\n",
    "X1 = X1.div(scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log_model = log_regr.fit(X1, target)\n",
    "dump(log_model, 'data/reduced_log_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model=load('data/log_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = log_model.predict(X)\n",
    "metrics.f1_score(y_true=target, y_pred=pred)\n",
    "metrics.precision_score(y_true=target, y_pred=pred)\n",
    "metrics.recall_score(y_true=target, y_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = log_model.predict_proba(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = X1.reset_index()['fullVisitorId'].to_frame().join(pd.DataFrame(proba_pred[:,1],columns=['probability']))\n",
    "rev_pred = pd.read_csv('RFR_prediction_train.csv', dtype={'fullVisitorId': 'str'})\n",
    "rev_pred.shape\n",
    "rev_pred.head()\n",
    "rev_pred = rev_pred.assign(log_ob = np.log(rev_pred['observed']+1))\n",
    "rev_pred = rev_pred.assign(log_pred = np.log(rev_pred['prediction']+1))\n",
    "pr = pd.merge(probs, rev_pred, on='fullVisitorId')\n",
    "pr.head()\n",
    "#compute predictedLogRevenue\n",
    "pr = pr.assign(exp = pr['probability']*pr['log_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(pr['log_ob'], pr['exp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model=load('data/log_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic model, 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "train = {}\n",
    "test = {}\n",
    "for train_index, test_index in skf.split(X, target):\n",
    "    train[counter] = train_index\n",
    "    test[counter] = test_index\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check proportions\n",
    "for key in train:\n",
    "    print((target[train[key]]==1).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(train, 'data/train.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(test, 'data/test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(train[1]).union(set(test[1])) == set([i for i in range(329636)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counter = 1\n",
    "models = {}\n",
    "for index in train:\n",
    "    log_model = log_regr.fit(df.loc[train[index]], target[train[index]])\n",
    "    models[f\"log_model_{counter}\"] = log_model\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_3 = models['log_model_3'].predict(X_norm.loc[test[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    proba_pred = models[f\"log_model_{i}\"].predict_proba(X_norm.loc[test[i]])\n",
    "    mse = metrics.mean_squared_error(y_pred = proba_pred[:,1], y_true=target[test[i]])\n",
    "    baseline = metrics.mean_squared_error(y_pred = np.zeros(len(target[test[i]])), y_true=target[test[i]])\n",
    "    print(f\"MSE: {mse}. Baseline MSE: {baseline}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_pred = models[f\"log_model_{1}\"].predict_proba(X_norm.loc[test[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[test[1]].set_index('fullVisitorId')[target == 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
