{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "Group project for the 2019 Data Science Workshop at the University of California, Berkeley.\n",
    "\n",
    "The project is the Google Analytics Customer Revenue Prediction competition on Kaggle: https://www.kaggle.com/c/ga-customer-revenue-prediction\n",
    "\n",
    "Group members:\n",
    "\n",
    "* Andy Vargas (mentor)\n",
    "* Yuem Park\n",
    "* Marvin Pohl\n",
    "* Michael Yeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.stats import linregress\n",
    "import datetime as dt\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Note that the data files are too large to upload to GitHub - instead, the directory `./data/` has been added to the .gitignore, which should contain the following files on your local machine, all downloaded from the Kaggle competition website:\n",
    "\n",
    "* sample_submission_v2.csv\n",
    "* test_v2.csv\n",
    "* train_v2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time windows we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = dt.datetime(2016, 8, 1)\n",
    "train_end_date = dt.datetime(2018, 4, 30)\n",
    "train_duration = train_end_date - train_start_date + dt.timedelta(days=1)\n",
    "print('train duration = {} days'.format(train_duration.days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_date = dt.datetime(2018, 5, 1)\n",
    "test_end_date = dt.datetime(2018, 10, 15)\n",
    "test_duration = test_end_date - test_start_date + dt.timedelta(days=1)\n",
    "print('test duration = {} days'.format(test_duration.days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_start_date = dt.datetime(2018, 12, 1)\n",
    "predict_end_date = dt.datetime(2019, 1, 31)\n",
    "predict_duration = predict_end_date - predict_start_date + dt.timedelta(days=1)\n",
    "print('predict duration = {} days'.format(predict_duration.days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_duration = predict_start_date - test_end_date - dt.timedelta(days=1)\n",
    "print('gap duration = {} days'.format(gap_duration.days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What fraction of the dataset is one test duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_duration.days / train_duration.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the last day we can start a training time window on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end_date - (test_duration + predict_duration + gap_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A middle time slice, to get 3 slices in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date + (((train_end_date - (test_duration + predict_duration + gap_duration)) - train_start_date) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_slice = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "\n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId':'str'}) # Important!!\n",
    "    \n",
    "    # fix the formatting in these two columns, and convert them into lists of dictionaries\n",
    "    df['hits'] = df['hits'].str.replace(\"{'\", '{\"')\n",
    "    df['hits'] = df['hits'].str.replace(\"'}\", '\"}')\n",
    "    df['hits'] = df['hits'].str.replace(\": '\", ': \"')\n",
    "    df['hits'] = df['hits'].str.replace(\"',\", '\",')\n",
    "    df['hits'] = df['hits'].str.replace(\", '\", ', \"')\n",
    "    df['hits'] = df['hits'].str.replace(\"':\", '\":')\n",
    "    df['hits'] = df['hits'].str.replace(\"\\'\", \"'\")\n",
    "    df['hits'] = df['hits'].str.replace('\"7\" ', '\"7in ')\n",
    "    df['hits'] = df['hits'].str.replace('/7\" ', '/7in ')\n",
    "    df['hits'] = df['hits'].str.replace('\"Player\"', \"'Player'\")\n",
    "    df['hits'] = df['hits'].str.replace('True', 'true')\n",
    "    df['hits'] = df['hits'].str.replace('False', 'false')\n",
    "    df['hits'] = df['hits'].apply(json.loads)\n",
    "    \n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\"{'\", '{\"')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\"'}\", '\"}')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\": '\", ': \"')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\"',\", '\",')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\", '\", ', \"')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\"':\", '\":')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace(\"\\'\", \"'\")\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace('True', 'true')\n",
    "    df['customDimensions'] = df['customDimensions'].str.replace('False', 'false')\n",
    "    df['customDimensions'] = df['customDimensions'].apply(json.loads)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_slice == True:\n",
    "    \n",
    "    # set slice dates here\n",
    "    slice_start_date = dt.datetime(2017,7,28)\n",
    "    slice_end_date = slice_start_date + test_duration - dt.timedelta(days=1)\n",
    "    predict_slice_start_date = slice_end_date + gap_duration - dt.timedelta(days=1)\n",
    "    predict_slice_end_date = predict_slice_start_date + predict_duration - dt.timedelta(days=1)\n",
    "    \n",
    "    # read in .csv, and covert the date column to datetime\n",
    "    train = pd.read_csv('./data/train_v2.csv',dtype={'fullVisitorId':'str'})\n",
    "    print('1/4 : Raw training read in.')\n",
    "    train_date = pd.to_datetime(train['date'], format='%Y%m%d')\n",
    "    \n",
    "    # slice out train dataframe\n",
    "    train_slice = train[(train_date>=slice_start_date) & (train_date<=slice_end_date)]\n",
    "    train_predict_slice = train[(train_date>=predict_slice_start_date) & (train_date<=predict_slice_end_date)]\n",
    "    \n",
    "    # reset indexes\n",
    "    train_slice.reset_index(inplace=True,drop=True)\n",
    "    train_predict_slice.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    # save to .csv\n",
    "    train_slice.to_csv('./data/train_slice_01.csv', index=False)\n",
    "    train_predict_slice.to_csv('./data/train_predict_slice_01.csv', index=False)\n",
    "    \n",
    "    # load .csv's back in, but using the loading function to flatten most of the JSON columns\n",
    "    train_slice = load_df('./data/train_slice_01.csv')\n",
    "    train_predict_slice = load_df('./data/train_predict_slice_01.csv')\n",
    "    print('2/4 : Sliced training read in.')\n",
    "    \n",
    "    # only keep the target column for the prediction slice\n",
    "    train_predict_slice = train_predict_slice[['fullVisitorId','totals.transactionRevenue']]\n",
    "    \n",
    "    # remove columns that have no information from the training slice\n",
    "    NA_cols = []\n",
    "    for col in train_slice.columns:\n",
    "        if col!='hits' and col!='customDimensions':\n",
    "            if train_slice[col].nunique(dropna=False)==1:\n",
    "                NA_cols.append(col)\n",
    "    train_slice.drop(NA_cols, axis=1, inplace=True)\n",
    "    print('3/4 : Bad columns dropped.')\n",
    "    \n",
    "    # save to .csv\n",
    "    train_slice.to_csv('./data/train_slice_cleaned_01.csv', index=False)\n",
    "    train_predict_slice.to_csv('.data/train_predict_slice_cleaned_01.csv', index=False)\n",
    "    print('4/4 : Cleaned data saved.')\n",
    "    \n",
    "else:\n",
    "    train_slice = pd.read_csv('./data/train_slice_cleaned_01.csv', dtype={'fullVisitorId':'str'})\n",
    "    train_predict_slice = pd.read_csv('./data/train_predict_slice_cleaned_01.csv', dtype={'fullVisitorId':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_slice.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to identify json load errors...:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "\n",
    "df = pd.read_csv(csv_path, \n",
    "                 converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                 dtype={'fullVisitorId':'str'}) # Important!!\n",
    "\n",
    "# fix the formatting in these two columns, and convert them into lists of dictionaries\n",
    "df['hits'] = df['hits'].str.replace(\"{'\", '{\"')\n",
    "df['hits'] = df['hits'].str.replace(\"'}\", '\"}')\n",
    "df['hits'] = df['hits'].str.replace(\": '\", ': \"')\n",
    "df['hits'] = df['hits'].str.replace(\"',\", '\",')\n",
    "df['hits'] = df['hits'].str.replace(\", '\", ', \"')\n",
    "df['hits'] = df['hits'].str.replace(\"':\", '\":')\n",
    "df['hits'] = df['hits'].str.replace(\"\\'\", \"'\")\n",
    "df['hits'] = df['hits'].str.replace('\"7\" ', '\"7in ')\n",
    "df['hits'] = df['hits'].str.replace('/7\" ', '/7in ')\n",
    "df['hits'] = df['hits'].str.replace('\"Player\"', \"'Player'\")\n",
    "df['hits'] = df['hits'].str.replace('True', 'true')\n",
    "df['hits'] = df['hits'].str.replace('False', 'false')\n",
    "df['hits'] = df['hits'].apply(json.loads)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.at[i,'hits'] = json.loads(df['hits'][i])\n",
    "    except Exception as e:\n",
    "        print(i)\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_small['hits'][53504][3900:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_small.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations:\n",
    "\n",
    "* what factors can be correlated to zero revenue vs. positive revenue?\n",
    "* what factors can be correlated to revenue specifically within the group that has positive revenue?\n",
    "\n",
    "Thoughts:\n",
    "\n",
    "* perhaps first predict the binary zero revenue vs. positive revenue first, then run a separate model that specifically predicts the magnitude of the revenue within the positive revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_target = train_small.groupby('fullVisitorId')['totals.transactionRevenue'].sum()\n",
    "train_small_target.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15,5))\n",
    "\n",
    "ax[0].bar([0,1],\n",
    "          [len(train_small_target[train_small_target==0]), len(train_small_target[train_small_target!=0])],\n",
    "          width=0.4, color=['C0','C1'])\n",
    "ax[0].set_xlim(-1,2)\n",
    "ax[0].set_xticks([0,1])\n",
    "ax[0].set_xticklabels(['zero revenue', 'positive revenue'])\n",
    "ax[0].set_ylabel('n')\n",
    "\n",
    "ax[1].hist(np.log10(train_small_target[train_small_target!=0]), color='C1')\n",
    "ax[1].set_xlabel('log$_{10}$(revenue)')\n",
    "ax[1].set_ylabel('n')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add flag that indicates if the visitor has zero or positive revenue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(len(train_small))):\n",
    "    if train_small_target[train_small['fullVisitorId'][i]] == 0:\n",
    "        train_small.loc[i,'revenue_flag'] = 0\n",
    "    else:\n",
    "        train_small.loc[i,'revenue_flag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_zero = train_small[train_small['revenue_flag']==0]\n",
    "train_small_pstv = train_small[train_small['revenue_flag']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_vs_pstv_bar(feature):\n",
    "    \"\"\"\n",
    "    Make bar plots that compare zero vs. positive revenue for categorical features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str\n",
    "        Name of feature.\n",
    "    \"\"\"\n",
    "    unique_vals = train_small[feature].unique()\n",
    "    n_vals = len(unique_vals)\n",
    "\n",
    "    val_counts = np.zeros(n_vals)\n",
    "    for i in range(n_vals):\n",
    "        if pd.isnull(unique_vals[i]):\n",
    "            val_counts[i] = len(train_small[train_small[feature].isnull()])\n",
    "        else:\n",
    "            val_counts[i] = len(train_small[train_small[feature]==unique_vals[i]])\n",
    "\n",
    "    val_counts_zero = np.zeros(n_vals)\n",
    "    for i in range(n_vals):\n",
    "        if pd.isnull(unique_vals[i]):\n",
    "            val_counts_zero[i] = len(train_small_zero[train_small_zero[feature].isnull()])\n",
    "        else:\n",
    "            val_counts_zero[i] = len(train_small_zero[train_small_zero[feature]==unique_vals[i]])\n",
    "\n",
    "    val_counts_pstv = np.zeros(n_vals)\n",
    "    for i in range(n_vals):\n",
    "        if pd.isnull(unique_vals[i]):\n",
    "            val_counts_pstv[i] = len(train_small_pstv[train_small_pstv[feature].isnull()])\n",
    "        else:\n",
    "            val_counts_pstv[i] = len(train_small_pstv[train_small_pstv[feature]==unique_vals[i]])\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "    ax[0].bar(np.arange(n_vals),val_counts,color='C2')\n",
    "    ax[0].set_xticks(np.arange(n_vals))\n",
    "    ax[0].set_xticklabels(unique_vals, rotation=90)\n",
    "    ax[0].set_title('all')\n",
    "\n",
    "    ax[1].bar(np.arange(n_vals),val_counts_zero,color='C0')\n",
    "    ax[1].set_xticks(np.arange(n_vals))\n",
    "    ax[1].set_xticklabels(unique_vals, rotation=90)\n",
    "    ax[1].set_title('zero')\n",
    "\n",
    "    ax[2].bar(np.arange(n_vals),val_counts_pstv,color='C1')\n",
    "    ax[2].set_xticks(np.arange(n_vals))\n",
    "    ax[2].set_xticklabels(unique_vals, rotation=90)\n",
    "    ax[2].set_title('positive')\n",
    "\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_vs_pstv_bar_OTHER(feature, unique_vals):\n",
    "    \"\"\"\n",
    "    Make bar plots that compare zero vs. positive revenue for categorical features, including OTHER.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : str\n",
    "        Name of feature.\n",
    "        \n",
    "    unique_vals : list\n",
    "        List of categories - must include 'OTHER' as the last item.\n",
    "    \"\"\"\n",
    "    n_vals = len(unique_vals)\n",
    "\n",
    "    val_counts = np.zeros(n_vals)\n",
    "    for i in range(n_vals):\n",
    "        if pd.isnull(unique_vals[i]):\n",
    "            val_counts[i] = len(train_small[train_small[feature].isnull()])\n",
    "        else:\n",
    "            if unique_vals[i] != 'OTHER':\n",
    "                val_counts[i] = len(train_small[train_small[feature]==unique_vals[i]])\n",
    "            else:\n",
    "                val_counts[i] = len(train_small) - np.sum(val_counts)\n",
    "\n",
    "    val_counts_zero = np.zeros(n_vals)\n",
    "    for i in range(n_vals):\n",
    "        if pd.isnull(unique_vals[i]):\n",
    "            val_counts_zero[i] = len(train_small_zero[train_small_zero[feature].isnull()])\n",
    "        else:\n",
    "            if unique_vals[i] != 'OTHER':\n",
    "                val_counts_zero[i] = len(train_small_zero[train_small_zero[feature]==unique_vals[i]])\n",
    "            else:\n",
    "                val_counts_zero[i] = len(train_small_zero) - np.sum(val_counts_zero)\n",
    "\n",
    "    val_counts_pstv = np.zeros(n_vals)\n",
    "    for i in range(n_vals):\n",
    "        if pd.isnull(unique_vals[i]):\n",
    "            val_counts_pstv[i] = len(train_small_pstv[train_small_pstv[feature].isnull()])\n",
    "        else:\n",
    "            if unique_vals[i] != 'OTHER':\n",
    "                val_counts_pstv[i] = len(train_small_pstv[train_small_pstv[feature]==unique_vals[i]])\n",
    "            else:\n",
    "                val_counts_pstv[i] = len(train_small_pstv) - np.sum(val_counts_pstv)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "    ax[0].bar(np.arange(n_vals),val_counts,color='C2')\n",
    "    ax[0].set_xticks(np.arange(n_vals))\n",
    "    ax[0].set_xticklabels(unique_vals, rotation=90)\n",
    "    ax[0].set_title('all')\n",
    "\n",
    "    ax[1].bar(np.arange(n_vals),val_counts_zero,color='C0')\n",
    "    ax[1].set_xticks(np.arange(n_vals))\n",
    "    ax[1].set_xticklabels(unique_vals, rotation=90)\n",
    "    ax[1].set_title('zero')\n",
    "\n",
    "    ax[2].bar(np.arange(n_vals),val_counts_pstv,color='C1')\n",
    "    ax[2].set_xticks(np.arange(n_vals))\n",
    "    ax[2].set_xticklabels(unique_vals, rotation=90)\n",
    "    ax[2].set_title('positive')\n",
    "\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### channelGrouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['channelGrouping'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('channelGrouping')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_small_pstv['channelGrouping'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.sum(train_small_pstv['totals.transactionRevenue'].isnull())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unique_cats = train_small_pstv['channelGrouping'].unique()\n",
    "\n",
    "bins = np.histogram(train_small_pstv['totals.transactionRevenue'], bins=40, density=True)[1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "for i in range(len(unique_cats)):\n",
    "    df_slice = train_small_pstv[train_small_pstv['channelGrouping']==unique_cats[i]]\n",
    "    ax.hist(np.log10(df_slice['totals.transactionRevenue']), bins=bins,\n",
    "            alpha=0.5, density=True)\n",
    "    \n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device.browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['device.browser'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_browser_value_counts = train_small['device.browser'].value_counts(dropna=False)\n",
    "device_browser_value_counts[device_browser_value_counts>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'device.browser'\n",
    "\n",
    "unique_vals = device_browser_value_counts[device_browser_value_counts>100].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device.deviceCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['device.deviceCategory'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('device.deviceCategory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device.isMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['device.isMobile'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('device.isMobile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device.operatingSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['device.operatingSystem'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('device.operatingSystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### geoNetwork.continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['geoNetwork.continent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('geoNetwork.continent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### geoNetwork.subContinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['geoNetwork.subContinent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('geoNetwork.subContinent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### geoNetwork.networkDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_small['geoNetwork.networkDomain'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoNetwork_networkDomain_value_counts = train_small['geoNetwork.networkDomain'].value_counts(dropna=False)\n",
    "geoNetwork_networkDomain_value_counts[geoNetwork_networkDomain_value_counts>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'geoNetwork.networkDomain'\n",
    "\n",
    "unique_vals = geoNetwork_networkDomain_value_counts[geoNetwork_networkDomain_value_counts>1000].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.adContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.adContent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficSource_adContent_value_counts = train_small['trafficSource.adContent'].value_counts(dropna=False)\n",
    "trafficSource_adContent_value_counts[trafficSource_adContent_value_counts>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'trafficSource.adContent'\n",
    "\n",
    "unique_vals = trafficSource_adContent_value_counts[trafficSource_adContent_value_counts>100].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.adwordsClickInfo.adNetworkType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.adwordsClickInfo.adNetworkType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('trafficSource.adwordsClickInfo.adNetworkType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.adwordsClickInfo.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.adwordsClickInfo.page'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('trafficSource.adwordsClickInfo.page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.adwordsClickInfo.slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.adwordsClickInfo.slot'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('trafficSource.adwordsClickInfo.slot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.adwordsClickInfo.gclId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.adwordsClickInfo.gclId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficSource_adwordsClickInfo_gclId_value_counts = train_small['trafficSource.adwordsClickInfo.gclId'].value_counts(dropna=False)\n",
    "trafficSource_adwordsClickInfo_gclId_value_counts[trafficSource_adwordsClickInfo_gclId_value_counts>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'trafficSource.adwordsClickInfo.gclId'\n",
    "\n",
    "unique_vals = trafficSource_adwordsClickInfo_gclId_value_counts[trafficSource_adwordsClickInfo_gclId_value_counts>3].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.medium'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vs_pstv_bar('trafficSource.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small['trafficSource.campaign'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficSource_campaign_value_counts = train_small['trafficSource.campaign'].value_counts(dropna=False)\n",
    "trafficSource_campaign_value_counts[trafficSource_campaign_value_counts>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'trafficSource.campaign'\n",
    "\n",
    "unique_vals = trafficSource_campaign_value_counts[trafficSource_campaign_value_counts>100].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficSource_keyword_value_counts = train_small['trafficSource.keyword'].value_counts(dropna=False)\n",
    "trafficSource_keyword_value_counts[trafficSource_keyword_value_counts>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'trafficSource.keyword'\n",
    "\n",
    "unique_vals = trafficSource_keyword_value_counts[trafficSource_keyword_value_counts>100].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.referralPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trafficSource_referralPath_value_counts = train_small['trafficSource.referralPath'].value_counts(dropna=False)\n",
    "trafficSource_referralPath_value_counts[trafficSource_referralPath_value_counts>500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'trafficSource.referralPath'\n",
    "\n",
    "unique_vals = trafficSource_referralPath_value_counts[trafficSource_referralPath_value_counts>500].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trafficSource.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficSource_source_value_counts = train_small['trafficSource.source'].value_counts(dropna=False)\n",
    "trafficSource_source_value_counts[trafficSource_source_value_counts>500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'trafficSource.source'\n",
    "\n",
    "unique_vals = trafficSource_source_value_counts[trafficSource_source_value_counts>500].index.to_list()\n",
    "unique_vals.append('OTHER')\n",
    "\n",
    "zero_vs_pstv_bar_OTHER(feature, unique_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_pstv_target = train_small_pstv.groupby('fullVisitorId')['totals.transactionRevenue'].sum()\n",
    "train_small_pstv_target.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_value_counts_all = pd.to_datetime(train_small['date'], format='%Y%m%d').value_counts()\n",
    "date_value_counts_all.sort_index(inplace=True)\n",
    "\n",
    "date_value_counts_zero = pd.to_datetime(train_small_zero['date'], format='%Y%m%d').value_counts()\n",
    "date_value_counts_zero.sort_index(inplace=True)\n",
    "\n",
    "date_value_counts_pstv = pd.to_datetime(train_small_pstv['date'], format='%Y%m%d').value_counts()\n",
    "date_value_counts_pstv.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(15,10), sharex=True)\n",
    "\n",
    "ax[0].scatter(date_value_counts_all.index, date_value_counts_all, color='C2')\n",
    "ax[0].set_title('all')\n",
    "\n",
    "ax[1].scatter(date_value_counts_zero.index, date_value_counts_zero, color='C0')\n",
    "ax[1].set_title('zero')\n",
    "\n",
    "ax[2].scatter(date_value_counts_pstv.index, date_value_counts_pstv, color='C1')\n",
    "ax[2].set_title('positive')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visitStartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitStartTime_value_counts_all = pd.to_datetime(train_small['visitStartTime'], unit='s').value_counts()\n",
    "visitStartTime_value_counts_all = visitStartTime_value_counts_all.index.hour\n",
    "\n",
    "visitStartTime_value_counts_zero = pd.to_datetime(train_small_zero['visitStartTime'], unit='s').value_counts()\n",
    "visitStartTime_value_counts_zero = visitStartTime_value_counts_zero.index.hour\n",
    "\n",
    "visitStartTime_value_counts_pstv = pd.to_datetime(train_small_pstv['visitStartTime'], unit='s').value_counts()\n",
    "visitStartTime_value_counts_pstv = visitStartTime_value_counts_pstv.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8,10), sharex=True)\n",
    "\n",
    "ax[0].hist(visitStartTime_value_counts_all, color='C2')\n",
    "ax[0].set_title('all')\n",
    "\n",
    "ax[1].hist(visitStartTime_value_counts_zero, color='C0')\n",
    "ax[1].set_title('zero')\n",
    "\n",
    "ax[2].hist(visitStartTime_value_counts_pstv, color='C1')\n",
    "ax[2].set_title('positive')\n",
    "\n",
    "ax[2].set_xlabel('hour of day')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_pstv.loc[:,'visitStartTime_hour'] = pd.to_datetime(train_small_pstv['visitStartTime'], unit='s').dt.hour.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "x = train_small_pstv['visitStartTime_hour']\n",
    "y = np.log10(train_small_pstv['totals.transactionRevenue'])\n",
    "\n",
    "x_line = np.array([np.min(x), np.max(x)])\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print('R^2    = {}'.format(r_value**2))\n",
    "print('p-val. = {}'.format(p_value))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, c='C0')\n",
    "ax.plot(x_line, x_line*slope + intercept, c='k', ls='--')\n",
    "\n",
    "ax.set_xlabel('visit start time (hour)')\n",
    "ax.set_ylabel('log$_{10}$(transaction revenue)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### totals.hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8,10), sharex=True)\n",
    "\n",
    "ax[0].hist(np.log10(train_small['totals.hits']), color='C2')\n",
    "ax[0].set_title('all')\n",
    "\n",
    "ax[1].hist(np.log10(train_small_zero['totals.hits']), color='C0')\n",
    "ax[1].set_title('zero')\n",
    "\n",
    "ax[2].hist(np.log10(train_small_pstv['totals.hits']), color='C1')\n",
    "ax[2].set_title('positive')\n",
    "\n",
    "ax[2].set_xlabel('log$_{10}$(hits)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_pstv_hits = train_small_pstv.groupby('fullVisitorId')['totals.hits'].sum()\n",
    "train_small_pstv_hits.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "x = np.log10(train_small_pstv_hits)\n",
    "y = np.log10(train_small_pstv_target)\n",
    "\n",
    "x_line = np.array([np.min(x), np.max(x)])\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print('R^2    = {}'.format(r_value**2))\n",
    "print('p-val. = {}'.format(p_value))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, c='C0')\n",
    "ax.plot(x_line, x_line*slope + intercept, c='k', ls='--')\n",
    "\n",
    "ax.set_xlabel('log$_{10}$(hits)')\n",
    "ax.set_ylabel('log$_{10}$(transaction revenue)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### totals.pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8,10), sharex=True)\n",
    "\n",
    "ax[0].hist(np.log10(train_small['totals.pageviews']), color='C2')\n",
    "ax[0].set_title('all')\n",
    "\n",
    "ax[1].hist(np.log10(train_small_zero['totals.pageviews']), color='C0')\n",
    "ax[1].set_title('zero')\n",
    "\n",
    "ax[2].hist(np.log10(train_small_pstv['totals.pageviews']), color='C1')\n",
    "ax[2].set_title('positive')\n",
    "\n",
    "ax[2].set_xlabel('log$_{10}$(page views)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_pstv_pageviews = train_small_pstv.groupby('fullVisitorId')['totals.pageviews'].sum()\n",
    "train_small_pstv_pageviews.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "x = np.log10(train_small_pstv_pageviews)\n",
    "y = np.log10(train_small_pstv_target)\n",
    "\n",
    "x_line = np.array([np.min(x), np.max(x)])\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print('R^2    = {}'.format(r_value**2))\n",
    "print('p-val. = {}'.format(p_value))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, c='C0')\n",
    "ax.plot(x_line, x_line*slope + intercept, c='k', ls='--')\n",
    "\n",
    "ax.set_xlabel('log$_{10}$(page views)')\n",
    "ax.set_ylabel('log$_{10}$(transaction revenue)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### totals.sessionQualityDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8,10), sharex=True)\n",
    "\n",
    "ax[0].hist(train_small['totals.sessionQualityDim'], color='C2')\n",
    "ax[0].set_title('all')\n",
    "\n",
    "ax[1].hist(train_small_zero['totals.sessionQualityDim'], color='C0')\n",
    "ax[1].set_title('zero')\n",
    "\n",
    "ax[2].hist(train_small_pstv['totals.sessionQualityDim'], color='C1')\n",
    "ax[2].set_title('positive')\n",
    "\n",
    "ax[2].set_xlabel('session quality')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_pstv_sessionQualityDim = train_small_pstv.groupby('fullVisitorId')['totals.sessionQualityDim'].sum()\n",
    "train_small_pstv_sessionQualityDim.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "x = train_small_pstv_sessionQualityDim\n",
    "y = np.log10(train_small_pstv_target)\n",
    "\n",
    "x_line = np.array([np.min(x), np.max(x)])\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print('R^2    = {}'.format(r_value**2))\n",
    "print('p-val. = {}'.format(p_value))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, c='C0')\n",
    "ax.plot(x_line, x_line*slope + intercept, c='k', ls='--')\n",
    "\n",
    "ax.set_xlabel('session quality')\n",
    "ax.set_ylabel('log$_{10}$(transaction revenue)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### totals.timeOnSite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(8,10), sharex=True)\n",
    "\n",
    "ax[0].hist(np.log10(train_small['totals.timeOnSite']), color='C2')\n",
    "ax[0].set_title('all')\n",
    "\n",
    "ax[1].hist(np.log10(train_small_zero['totals.timeOnSite']), color='C0')\n",
    "ax[1].set_title('zero')\n",
    "\n",
    "ax[2].hist(np.log10(train_small_pstv['totals.timeOnSite']), color='C1')\n",
    "ax[2].set_title('positive')\n",
    "\n",
    "ax[2].set_xlabel('log$_{10}$(time on site)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_pstv_timeOnSite = train_small_pstv.groupby('fullVisitorId')['totals.timeOnSite'].sum()\n",
    "train_small_pstv_timeOnSite.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "x = np.log10(train_small_pstv_timeOnSite)\n",
    "y = np.log10(train_small_pstv_target)\n",
    "\n",
    "x_line = np.array([np.min(x), np.max(x)])\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "print('R^2    = {}'.format(r_value**2))\n",
    "print('p-val. = {}'.format(p_value))\n",
    "\n",
    "ax.scatter(x, y, alpha=0.2, c='C0')\n",
    "ax.plot(x_line, x_line*slope + intercept, c='k', ls='--')\n",
    "\n",
    "ax.set_xlabel('log$_{10}$(time on site)')\n",
    "ax.set_ylabel('log$_{10}$(transaction revenue)')\n",
    "\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
